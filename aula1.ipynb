{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instale os requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for thiago: Sorry, try again.\n",
      "[sudo] password for thiago: \n",
      "sudo: no password was provided\n",
      "sudo: 1 incorrect password attempt\n",
      "[sudo] password for thiago: ^C\n"
     ]
    }
   ],
   "source": [
    "!chmod u=rwx,g=rx,o=rx  start.sh && ./start.sh -p \"zTAgUdbQ&\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"./spark-3.1.2-bin-hadoop2.7\"\n",
    "findspark.init()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [SparkSession](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html)\n",
    "\n",
    "O ponto de entrada para programar o Spark com a API Dataset e DataFrame.\n",
    "\n",
    "Uma SparkSession pode ser utilizada para criar DataFrames, registrar DataFrames como tabelas, executar consultas SQL em tabelas, armazenar em cache e ler arquivos parquet. Para criar uma SparkSession, use o seguinte padrÃ£o de construtor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder \\\n\u001b[1;32m      4\u001b[0m     \u001b[39m.\u001b[39mmaster(\u001b[39m'\u001b[39m\u001b[39mlocal[*]\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mIniciando com Spark\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[39m.\u001b[39mgetOrCreate()\n\u001b[1;32m      8\u001b[0m spark\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Iniciando com Spark\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-25ubuntu1.1).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  libfwupdplugin1 libxmlb1\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
      "UnZip 6.00 of 20 April 2009, by Debian. Original by Info-ZIP.\n",
      "\n",
      "Latest sources and executables are at ftp://ftp.info-zip.org/pub/infozip/ ;\n",
      "see ftp://ftp.info-zip.org/pub/infozip/UnZip.html for other sites.\n",
      "\n",
      "Compiled with gcc 9.4.0 for Unix (Linux ELF).\n",
      "\n",
      "UnZip special compilation options:\n",
      "        ACORN_FTYPE_NFS\n",
      "        COPYRIGHT_CLEAN (PKZIP 0.9x unreducing method not supported)\n",
      "        SET_DIR_ATTRIB\n",
      "        SYMLINKS (symbolic links supported, if RTL and file system permit)\n",
      "        TIMESTAMP\n",
      "        UNIXBACKUP\n",
      "        USE_EF_UT_TIME\n",
      "        USE_UNSHRINK (PKZIP/Zip 1.x unshrinking method supported)\n",
      "        USE_DEFLATE64 (PKZIP 4.x Deflate64(tm) supported)\n",
      "        UNICODE_SUPPORT [wide-chars, char coding: UTF-8] (handle UTF-8 paths)\n",
      "        LARGE_FILE_SUPPORT (large files over 2 GiB supported)\n",
      "        ZIP64_SUPPORT (archives using Zip64 for large files supported)\n",
      "        USE_BZIP2 (PKZIP 4.6+, using bzip2 lib version 1.0.8, 13-Jul-2019)\n",
      "        VMS_TEXT_CONV\n",
      "        WILD_STOP_AT_DIR\n",
      "        [decryption, version 2.11 of 05 Jan 2007]\n",
      "\n",
      "UnZip and ZipInfo environment options:\n",
      "           UNZIP:  [none]\n",
      "        UNZIPOPT:  [none]\n",
      "         ZIPINFO:  [none]\n",
      "      ZIPINFOOPT:  [none]\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "  inflating: ngrok                   \n",
      "[INFO] Ngrok instalation done!\n"
     ]
    }
   ],
   "source": [
    "!chmod u=rwx,g=rx,o=rx  ngrok_install.sh && ./ngrok_install.sh -p \"your_sudo_password\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
