{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instale os requisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://apt.releases.hashicorp.com focal InRelease                       \n",
      "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease                         \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease                 \n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]      \n",
      "Get:5 https://packages.microsoft.com/ubuntu/20.04/prod focal InRelease [3611 B]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \n",
      "Fetched 226 kB in 3s (89.8 kB/s)                                    \n",
      "Reading package lists... Done\n",
      "--2023-06-24 11:20:57--  https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
      "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
      "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 224445805 (214M) [application/x-gzip]\n",
      "Saving to: ‘spark-3.1.2-bin-hadoop2.7.tgz’\n",
      "\n",
      "spark-3.1.2-bin-had 100%[===================>] 214.05M  1.53MB/s    in 2m 27s  \n",
      "\n",
      "2023-06-24 11:23:26 (1.45 MB/s) - ‘spark-3.1.2-bin-hadoop2.7.tgz’ saved [224445805/224445805]\n",
      "\n",
      "Requirement already satisfied: findspark in /home/thiago/.local/lib/python3.8/site-packages (2.0.1)\n",
      "[INFO] Installation done!\n"
     ]
    }
   ],
   "source": [
    "!chmod u=rwx,g=rx,o=rx  start.sh && ./start.sh -p \"your_sudo_password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"./spark-3.1.2-bin-hadoop2.7\"\n",
    "findspark.init()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [SparkSession](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html)\n",
    "\n",
    "O ponto de entrada para programar o Spark com a API Dataset e DataFrame.\n",
    "\n",
    "Uma SparkSession pode ser utilizada para criar DataFrames, registrar DataFrames como tabelas, executar consultas SQL em tabelas, armazenar em cache e ler arquivos parquet. Para criar uma SparkSession, use o seguinte padrão de construtor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/24 11:27:02 WARN Utils: Your hostname, W10-tks resolves to a loopback address: 127.0.1.1; using 172.25.91.212 instead (on interface eth0)\n",
      "23/06/24 11:27:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/06/24 11:27:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.25.91.212:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Iniciando com Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8447785d00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Iniciando com Spark\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-25ubuntu1.1).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  libfwupdplugin1 libxmlb1\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
      "UnZip 6.00 of 20 April 2009, by Debian. Original by Info-ZIP.\n",
      "\n",
      "Latest sources and executables are at ftp://ftp.info-zip.org/pub/infozip/ ;\n",
      "see ftp://ftp.info-zip.org/pub/infozip/UnZip.html for other sites.\n",
      "\n",
      "Compiled with gcc 9.4.0 for Unix (Linux ELF).\n",
      "\n",
      "UnZip special compilation options:\n",
      "        ACORN_FTYPE_NFS\n",
      "        COPYRIGHT_CLEAN (PKZIP 0.9x unreducing method not supported)\n",
      "        SET_DIR_ATTRIB\n",
      "        SYMLINKS (symbolic links supported, if RTL and file system permit)\n",
      "        TIMESTAMP\n",
      "        UNIXBACKUP\n",
      "        USE_EF_UT_TIME\n",
      "        USE_UNSHRINK (PKZIP/Zip 1.x unshrinking method supported)\n",
      "        USE_DEFLATE64 (PKZIP 4.x Deflate64(tm) supported)\n",
      "        UNICODE_SUPPORT [wide-chars, char coding: UTF-8] (handle UTF-8 paths)\n",
      "        LARGE_FILE_SUPPORT (large files over 2 GiB supported)\n",
      "        ZIP64_SUPPORT (archives using Zip64 for large files supported)\n",
      "        USE_BZIP2 (PKZIP 4.6+, using bzip2 lib version 1.0.8, 13-Jul-2019)\n",
      "        VMS_TEXT_CONV\n",
      "        WILD_STOP_AT_DIR\n",
      "        [decryption, version 2.11 of 05 Jan 2007]\n",
      "\n",
      "UnZip and ZipInfo environment options:\n",
      "           UNZIP:  [none]\n",
      "        UNZIPOPT:  [none]\n",
      "         ZIPINFO:  [none]\n",
      "      ZIPINFOOPT:  [none]\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "  inflating: ngrok                   \n",
      "[INFO] Ngrok instalation done!\n"
     ]
    }
   ],
   "source": [
    "# !chmod u=rwx,g=rx,o=rx  ngrok_install.sh && ./ngrok_install.sh -p \"your_sudo_password\"\n",
    "!chmod u=rwx,g=rx,o=rx  ngrok_install.sh && ./ngrok_install.sh -p \"zTAgUdbQ&\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
